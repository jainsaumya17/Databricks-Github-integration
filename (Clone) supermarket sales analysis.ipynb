{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214d1421-886e-4cd2-865c-6115d3ccfcc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# accessing the secrets using dbutils.secrets and storing it in variable\n",
    "access_key = dbutils.secrets.get(scope=\"<aws_credentials>\", key=\"<aws_access_key>\")\n",
    "secret_key = dbutils.secrets.get(scope=\"<aws_credentials>\", key=\"<aws_secret_key>\")\n",
    "\n",
    "# creating a spark configuration to estaibilish connection with s3\n",
    "spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "aws_region = \"us-east-1\"\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73417e18-a1d0-47a3-9a6c-4563e8fa5b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"s3a://intuz-trainee-saumya-jain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8a33a8-56e4-4f88-b334-342e69fed17a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# removing file from s3 bucket \n",
    "#dbutils.fs.rm('s3a://intuz-trainee-saumya-jain/supermarket_sales_parquet', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9c27da-66b8-4c59-8f2d-d11f47af75f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading data from s3 bucket \n",
    "df_s3 = spark.read.option(\"header\",True).parquet(\"s3a://intuz-trainee-saumya-jain/supermarket_sales_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4783b808-272c-4adc-ba2b-9332262328be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4263c975-4606-43f0-83a0-83effedb7989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_s3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48dafe4-858e-4d3d-896a-183ba1b38001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "BRONZE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd6c753-b9ab-4333-8782-2e223977e6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table(\n",
    "  name = \"supermarket_bronze\",\n",
    "comment= \"This table contains raw supermarket data\")\n",
    "\n",
    "def supermarket_bronze():\n",
    "  df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferschema\",True)\n",
    "    # .schema(schema)\n",
    "    .load(\"s3a://intuz-trainee-saumya-jain/supermarket_sales_parquet\")\n",
    "  )\n",
    "  df = (df.withColumnRenamed(\"Invoice ID\",\"Invoice_ID\")\n",
    "    .withColumnRenamed(\"Customer type\",\"Customer_type\")\n",
    "    .withColumnRenamed(\"Product line\", \"Product_line\")\n",
    "    .withColumnRenamed(\"Unit price\", \"Unit_price\")\n",
    "    .withColumnRenamed(\"Tax 5%\", \"Tax_5_percent\")\n",
    "    .withColumnRenamed(\"gross margin percentage\", \"gross_margin_percentage\")\n",
    "    .withColumnRenamed(\"gross income\", \"gross_income\"))\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351cdec5-07a5-4b5f-b27a-d229b9afff7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col,to_timestamp\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"bronze_view\",\n",
    "    comment=\"This view contains raw supermarket data\"\n",
    ")\n",
    "def bronze_view():\n",
    "\n",
    "    df = dlt.read(\"supermarket_bronze\")\n",
    "\n",
    "    # changing data types of columns\n",
    "    df = (df.withColumn(\"Unit_price\", df[\"Unit_price\"].cast(\"float\"))\n",
    "          .withColumn(\"Tax_5_percent\", df[\"Tax_5_percent\"].cast(\"float\"))\n",
    "          .withColumn(\"Quantity\", df[\"Quantity\"].cast(\"int\"))\n",
    "          .withColumn(\"Total\", df[\"Total\"].cast(\"float\"))\n",
    "          .withColumn(\"cogs\", df[\"cogs\"].cast(\"float\"))\n",
    "          .withColumn(\"gross_margin_percentage\", df[\"gross_margin_percentage\"].cast(\"float\"))\n",
    "          .withColumn(\"gross_income\", df[\"gross_income\"].cast(\"float\"))\n",
    "          .withColumn(\"Rating\", df[\"Rating\"].cast(\"float\"))\n",
    "          .withColumn(\"Date\", to_date(col(\"Date\"), \"M/d/yyyy\"))\n",
    "          .withColumn(\"Time\", to_timestamp(col(\"Time\"), \"HH:mm\")))\n",
    "    \n",
    "    df = df.drop(col(\"_rescued_data\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b326df40-af4a-438a-a304-2440f8f06f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SILVER LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6ae997-a293-4064-aa35-b4ee611a52fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col,to_timestamp,date_format\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# creating silver table \n",
    "\n",
    "@dlt.table(\n",
    "  name = \"supermarket_silver\",\n",
    "  comment=\"this table contains processed supermarket data\"\n",
    ")\n",
    "\n",
    "def supermarket_silver():\n",
    "  df = spark.read.table(\"bronze_view\")\n",
    "  \n",
    "  # adding new columns in silver table \n",
    "  df = df.withColumn(\n",
    "    \"satisfaction_tier\",\n",
    "    when(col(\"rating\") >= 7, \"High\")\n",
    "    .when(col(\"rating\") >= 5, \"Medium\")\n",
    "    .otherwise(\"Low\"),  # All other ratings are considered \"Low\"\n",
    ")\n",
    "  df = df.withColumn(\"SalesMonth\",date_format(col(\"Date\"), \"MMM\"))  # Month\n",
    "  \n",
    "  return df\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37833356-d0ae-45db-b940-f90c6a04e55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT * FROM saumyaj_catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6397fce-fc17-4ab1-acf8-1ebb062309e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating silver view \n",
    "\n",
    "import dlt\n",
    "\n",
    "@dlt.view(\n",
    "    name = \"silver_view\",\n",
    "    comment=\"This is silver view with processed supermarket data\"\n",
    ")\n",
    "\n",
    "def silver_view():\n",
    "    df = dlt.read(\"supermarket_silver\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea85e679-117d-4326-a3b1-7f645f05b41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GOLD LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf96203-1432-4f0f-9114-93456001d790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GOLD TABLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014e74d2-f6c5-48e9-a885-061cb2c94b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@dlt.create_table( # use create_table decorator\n",
    "    name = \"monthly_sales\",\n",
    "    comment=\"This table contains sum of sales in each month\"\n",
    ")\n",
    "\n",
    "def monthly_sales():\n",
    "    df = spark.read.table(\"silver_view\")\n",
    "    df = df.withColumn(\"Total\", col(\"Total\").cast(DoubleType())) # Casting to DoubleType\n",
    "\n",
    "    # Sales Trends Over Time (Monthly)\n",
    "    monthly_sales = (\n",
    "        df.groupBy(\"SalesMonth\")\n",
    "        .agg(sf.sum(\"Total\").alias(\"MonthlySales\"))\n",
    "    )\n",
    "    return monthly_sales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678629ee-808c-4546-a3fa-c2b7301e9bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GOLD TABLE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b66cc1-1de0-4767-87c7-58170125ebf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "@dlt.create_table( # use create_table decorator\n",
    "    name = \"common_product_lines\",\n",
    "    comment=\"This table contains the most common product lines purchased in each city\"\n",
    ")\n",
    "\n",
    "def common_product_lines():\n",
    "    df = spark.read.table(\"silver_view\")\n",
    "\n",
    "\n",
    "    # common product lines purchased in each city\n",
    "    # city_wise_product_line = (\n",
    "    #     df.groupBy(\"City\",\"Product_line\")\n",
    "    #     .agg(sf.count(\"Product_line\").alias(\"Common_product_lines\"))\n",
    "    #     .select(\"CIty\",\"Product_line\").filter(\"Product_line\")\n",
    "    # )\n",
    "    # return city_wise_product_line\n",
    "\n",
    "    # 1. Group by City and Product_line and count the occurrences\n",
    "    product_line_counts = df.groupBy(\"City\", \"Product_line\").agg(sf.count(\"Product_line\").alias(\"pl_count\"))\n",
    "\n",
    "    # 2. Define a WindowSpec to partition by City and order by pl_count descending\n",
    "    window_spec = Window.partitionBy(\"City\").orderBy(col(\"pl_count\").desc())\n",
    "\n",
    "    # 3. Use row_number() to assign a rank to each product line within each city\n",
    "    ranked_product_lines = product_line_counts.withColumn(\"rn\", sf.row_number().over(window_spec))\n",
    "\n",
    "    # 4. Filter to get only the top product line (rank = 1) for each city\n",
    "    top_product_lines = ranked_product_lines.filter(col(\"rn\") == 1).select(\"City\", \"Product_line\", \"pl_count\")\n",
    "\n",
    "    return top_product_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d871ff98-8786-4923-a1c4-df950a58f55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "GOLD TABLE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8eb278-2671-444d-9380-72c841745618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# How does customer type (Member vs. Normal) influence total spending across different branches?\n",
    "\n",
    "@dlt.create_table( # use create_table decorator\n",
    "    name = \"customer_type_influence\",\n",
    "    comment=\"This table contains customer type influence on total spending across different branches\"\n",
    ")\n",
    "\n",
    "def customer_type_influence():\n",
    "    df = spark.read.table(\"silver_view\")\n",
    "\n",
    "    # Customer type influence on sales across different branches\n",
    "    customer_type_sales = (\n",
    "        df.groupBy(\"Branch\", \"Customer_type\")\n",
    "        .agg(sf.sum(\"Total\").alias(\"TotalSpending\"))\n",
    "        .orderBy(\"Branch\", \"Customer_type\")\n",
    "    )\n",
    "    return customer_type_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c38c33-4229-466b-989f-b6696df4507a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # -- SELECT City,Product_line,MAX(count(pl_count))\n",
    "# # -- FROM (\n",
    "# # --   SELECT City,Product_line,COUNT(Product_line) AS pl_count FROM saumyaj_catalog.supermarket_schema.supermarket_silver \n",
    "# # -- GROUP BY City,Product_line\n",
    "# # -- ORDER BY COUNT(Product_line)\n",
    "# # -- )\n",
    "# # -- GROUP BY City,Product_line;\n",
    "\n",
    "\n",
    "# -- WITH RankedProductLines AS (\n",
    "# --   SELECT \n",
    "# --     City, \n",
    "# --     Product_line, \n",
    "# --     COUNT(Product_line) AS pl_count,\n",
    "# --     ROW_NUMBER() OVER (PARTITION BY City ORDER BY COUNT(Product_line) DESC) AS rn\n",
    "# --   FROM saumyaj_catalog.supermarket_schema.supermarket_silver\n",
    "# --   GROUP BY City, Product_line\n",
    "# -- )\n",
    "# -- SELECT City, Product_line, pl_count\n",
    "# -- FROM RankedProductLines\n",
    "# -- WHERE rn = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4316bee-dfae-46e1-88d5-d34cc46e6995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT City, Count(City)\n",
    "# FROM saumyaj_catalog.supermarket_schema.supermarket_silver\n",
    "# GROUP BY City;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088d7ab1-58d0-4af4-aa62-1632e57cb435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dummy_df = spark.read.csv(\n",
    "    \"dbfs:/FileStore/tables/dummy_supermarket_sales.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "display(dummy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16e4563-0554-4370-be80-9413734039f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dummy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ae3bb5-d438-454e-ba4a-9295c316dc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adding more data in s3 bucket \n",
    "# dummy_df.write.mode(\"append\").parquet(\"s3a://intuz-trainee-saumya-jain/supermarket_sales_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462c00a6-7e54-47c5-933f-355ce94c8677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#updated_s3 = spark.read.parquet(\"s3a://intuz-trainee-saumya-jain/supermarket_sales_parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) supermarket sales analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
